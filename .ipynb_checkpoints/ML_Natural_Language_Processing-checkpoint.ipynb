{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "%logstop\n",
    "%logstart -rtq ~/.logs/ML_Natural_Language_Processing.py append\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "matplotlib.rcParams['figure.dpi'] = 144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural language processing (NLP) is the field devoted to methods and algorithms for processing human (natural) languages for computers. NLP is a vast discipline that is actively being researched. For this notebook, we will be concerned with NLP tools and techniques we can use for machine learning applications. Some examples of machine learning applications using NLP include sentiment analysis, topic modeling, and language translation. In NLP, the following terms have specific meanings:\n",
    "\n",
    "* **Corpus**: The body/collection of text being investigated.\n",
    "* **Document**: The unit of analysis, what is considered a single observation.\n",
    "\n",
    "Examples of corpora include a collection of reviews and tweets, the text of the _Iliad_, and Wikipedia articles. Documents can be whatever you decided, it is what your model will consider an observation. For the example when the corpus is a collection of reviews or tweets, it is logical to make the document a single review or tweet. For the example of the text of the _Iliad_, we can set the document size to a sentence or a paragraph. The choice of document size will be influenced by the size of our corpus. If it is large, it may make sense to call each paragraph a document. As is usually the case, some design choices that need to be made.\n",
    "\n",
    "For this notebook, we will build a classifier to discern homonyms, words that are spelled the same but that have different meanings. The exact use case we will explore is to discern if the word \"python\" refers to the programming language or the animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with spaCy\n",
    "\n",
    "spaCy is a Python package that bills itself as \"industrial-strength\" natural language processing. We will use the tools spaCy provides in conjunction with `scikit-learn`. Let's explore some of spaCy's capabilities; we will introduce more functionality when needed. More about spaCy can be found [here](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's try out spacy.\n",
      "We can easily divide our text into sentences!\n",
      "I've run out of ideas.\n",
      "Let\n",
      "ideas\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load text processing pipeline\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# nlp accepts a string\n",
    "doc = nlp(\"Let's try out spacy. We can easily divide our text into sentences! I've run out of ideas.\")\n",
    "\n",
    "# iterate through each sentence\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "\n",
    "# index words\n",
    "print(doc[0])\n",
    "print(doc[-2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another nice feature from spaCy is part-of-speech tagging, the process of identifying whether a word is a noun, adjective, adverb, etc. A processed word has the attribute `pos_` and `tag_`; the former identifies the simple part of speech (e.g., noun) wile the latter identifies the more detailed part of speech (e.g., proper noun). The meaning of the resulting abbreviations of the `tag_` are listed [here](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) or can be revealed by running `spacy.explain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DET', 'DT')\n",
      "('quick', 'ADJ', 'JJ')\n",
      "('brown', 'ADJ', 'JJ')\n",
      "('fox', 'NOUN', 'NN')\n",
      "('jumped', 'VERB', 'VBD')\n",
      "('over', 'ADP', 'IN')\n",
      "('the', 'DET', 'DT')\n",
      "('lazy', 'ADJ', 'JJ')\n",
      "('dog', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "('Mr.', 'PROPN', 'NNP')\n",
      "('Peanut', 'PROPN', 'NNP')\n",
      "('wears', 'VERB', 'VBZ')\n",
      "('a', 'DET', 'DT')\n",
      "('top', 'ADJ', 'JJ')\n",
      "('hat', 'NOUN', 'NN')\n",
      "('.', 'PUNCT', '.')\n",
      "\n",
      "NN noun, singular or mass\n",
      "VBD verb, past tense\n",
      "JJ adjective\n",
      "VBZ verb, 3rd person singular present\n",
      "DT determiner\n",
      "NNP noun, proper singular\n",
      ". punctuation mark, sentence closer\n",
      "IN conjunction, subordinating or preposition\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"The quick brown fox jumped over the lazy dog. Mr. Peanut wears a top hat.\")\n",
    "tags = set()\n",
    "\n",
    "# reveal part of speech\n",
    "for word in doc:\n",
    "    tags.add(word.tag_)\n",
    "    print((word.text, word.pos_, word.tag_))\n",
    "\n",
    "# revealing meaning of tags\n",
    "print()\n",
    "for tag in tags:\n",
    "    print(tag, spacy.explain(tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining a corpus\n",
    "\n",
    "Before we can move on with our analysis, we need to obtain a corpus. For our intended classifier, we need documents pertaining to python the animal and Python the programming language. Let's use Wikipedia articles to form our corpus. Luckily, there's a Python package called `wikipedia` that makes it easy to fetch articles. We will create documents based on the sentences in the articles. The function allows us to pass multiples pages in constructing the documents, allowing us to prevent one class of documents from dominating the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wikipedia\n",
    "\n",
    "# corpus = [sent for sent in nlp(page.content).sents for page in wikipedia.page(\"Nigeria\") ]\n",
    "# corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Python is an interpreted, high-level, general-purpose programming language.', \"Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes code readability with its notable use of significant whitespace.\", 'Its language constructs and object-oriented approach aim to help programmers write clear, logical code for small and large-scale projects.', 'Python is dynamically typed and garbage-collected.', 'It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented, and functional programming.']\n",
      "\n",
      "['The reticulated python (Malayopython reticulatus) is a snake species in the family Pythonidae native to South and Southeast Asia.', \"It is the world's longest snake and listed as least concern on the IUCN Red List because of its wide distribution.\", 'In several range countries, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.', 'It is an excellent swimmer, has been reported far out at sea and has colonized many small islands within its range.\\n', 'It is among the three heaviest snakes.']\n",
      "\n",
      "['The reticulated python (Malayopython reticulatus) is a snake species in the family Pythonidae native to South and Southeast Asia.', \"It is the world's longest snake and listed as least concern on the IUCN Red List because of its wide distribution.\", 'In several range countries, it is hunted for its skin, for use in traditional medicine, and for sale as a pet.', 'It is an excellent swimmer, has been reported far out at sea and has colonized many small islands within its range.\\n', 'It is among the three heaviest snakes.', 'Like all pythons, it is a non-venomous constrictor.', 'People have been killed (and in at least two reported cases, eaten) by reticulated pythons.\\n\\n\\n==', 'Taxonomy ==\\n', 'The reticulated python was first described in 1801 by  German naturalist Johann Gottlob Theaenus Schneider, who described two zoological specimens held by the Göttingen Museum in 1801 that differed slightly in colour and pattern as separate species—Boa reticulata and Boa rhombeata.  ', 'The specific name, reticulatus, is Latin meaning \"net-like\", or reticulated, and is a reference to the complex color pattern.', 'The generic name Python was proposed by French naturalist François Marie Daudin in 1803.', 'American zoologist Arnold G. Kluge performed a cladistics analysis on morphological characters and recovered the reticulated python lineage as sister to the genus Python, hence not requiring a new generic name in 1993.In a 2004 genetics study using cytochrome b DNA, Robin Lawson and colleagues discovered the reticulated python as sister to Australo-Papuan pythons, rather than Python molurus and relatives.', 'Raymond Hoser erected the genus for the reticulated python in 2004, naming it after German snake expert Stefan Broghammer, on the basis of dorsal patterns distinct from those of the genus Python, and a dark mid-dorsal line from the rear to the front of the head, and red or orange (rather than brown) iris colour.', \"In 2008, Lesley Rawlings and colleagues reanalysed Kruge's morphological data and combined it with genetic material, and found the reticulated clade to be an offshoot of the Australo-Papuan lineage as well.\", 'They adopted and redefined the genus name Broghammerus.', 'However, this and numerous other names by the same author were criticized by several authors, who proposed ignoring them for the purposes of nomenclature.', 'Reynolds and colleagues subsequently described the genus Malayopython for this species and its sister species, the Timor python, calling the Timor python M. timoriensis.', 'Hoser has since said that the Malayopython name is a junior synonym of Broghammerus, thus it should not be recognized by the International Code of Zoological Nomenclature.', 'Neither of these proposed reclassifications has been recognized by the ITIS, but Malayopython has been recognized by a number of subsequent authors and the Reptile Database.\\n\\n\\n=== Subspecies ===\\nThree subspecies have been proposed:\\n\\nM. r. reticulatus (Schneider, 1801) -', 'Asiatic reticulated python\\n']\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "\n",
    "def pages_to_sentences(*pages):\n",
    "    \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "animal_sents = pages_to_sentences(\"Reticulated python\", \"Ball Python\")\n",
    "language_sents = pages_to_sentences(\"Python (programming language)\")\n",
    "documents = animal_sents + language_sents\n",
    "\n",
    "print(language_sents[:5])\n",
    "print()\n",
    "print(animal_sents[:5])\n",
    "print()\n",
    "print(documents[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(documents)\n",
    "len(animal_sents)\n",
    "# len(language_sents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**\n",
    "* Given the example documents, what patterns should our word usage classifier learn?\n",
    "* We chose to create documents from sentences. What are other options? What are some pros and cons?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words model\n",
    "\n",
    "Machine learning models needs to ingest data in a structured form, a matrix where the rows represents observations and the columns are features/attributes. When working with text data, we need a method to convert this unstructured data into a form that the machine learning model can work with. Let's consider our motivating example to create a classifier to discern the usage of \"python\" in a document. We understand that documents referring to the programming language will use words such as \"integer\", \"byte\", and \"error\" at higher frequency than documents that refer to python the animal. The reverse is true for words such as \"bite\", \"snake\", and \"pet\". One technique to _transform_ text data into a matrix is to count the number of appearances of each word in each document. This technique is called the **bag of words** model. The model gets its name because each document is viewed as a bag holding all the words, disregarding word order, context, and grammar. After applying the bag of words model to a corpus, the resulting matrix will exhibit patterns that a machine learning model can exploit. See the example below for the result of applying the bag of words model to a corpus of two documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 0: \"The python is a large snake, although the snake is not venomous.\" <br>\n",
    "Document 1: \"Python is an interpreted programming language for general purpose programming.\" <br>\n",
    "<br>\n",
    "\n",
    "| although | an | for | general | interpreted | is | language | large | not | programming | purpose | python | snake | the | venomous |\n",
    "|:--------:|----|-----|---------|-------------|----|----------|-------|-----|-------------|---------|--------|-------|-----|----------|\n",
    "|     1    | 0  | 0   | 0       | 0           | 2  | 0        | 1     | 1   | 0           | 0       | 1      | 2     | 2   | 1        |\n",
    "|     0    | 1  | 1   | 1       | 1           | 1  | 1        | 0     | 0   | 2           | 1       | 1      | 0     | 0   | 0        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `CountVectorizer` transformer\n",
    "\n",
    "The bag of words model is found in `scikit-learn` with the `CountVectorizer` transformer. Note, `scikit-learn` uses the word `Vectorizer` to refer to transformers that convert a data structure (like a dictionary) into a NumPy array. Since it is a transformer, we need to first fit the object and _then_ call `transform`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 227)\t1\n",
      "  (0, 278)\t1\n",
      "  (0, 986)\t1\n",
      "  (0, 1278)\t1\n",
      "  (0, 1370)\t1\n",
      "  (0, 1545)\t1\n",
      "  (0, 1698)\t1\n",
      "  (0, 2027)\t1\n",
      "  (0, 2030)\t1\n",
      "  (0, 2150)\t1\n",
      "  (0, 2152)\t1\n",
      "  (0, 2349)\t1\n",
      "  (0, 2371)\t1\n",
      "  (0, 2372)\t1\n",
      "  (0, 2382)\t1\n",
      "  (0, 2554)\t2\n",
      "  (0, 2591)\t1\n",
      "  (1, 227)\t1\n",
      "  (1, 277)\t1\n",
      "  (1, 343)\t1\n",
      "  (1, 599)\t1\n",
      "  (1, 796)\t1\n",
      "  (1, 1370)\t1\n",
      "  (1, 1375)\t1\n",
      "  (1, 1387)\t1\n",
      "  :\t:\n",
      "  (607, 1371)\t1\n",
      "  (608, 51)\t1\n",
      "  (608, 1570)\t1\n",
      "  (608, 2484)\t1\n",
      "  (609, 71)\t1\n",
      "  (609, 844)\t1\n",
      "  (609, 1278)\t1\n",
      "  (609, 1985)\t1\n",
      "  (609, 2027)\t1\n",
      "  (610, 161)\t1\n",
      "  (610, 1978)\t1\n",
      "  (610, 2750)\t1\n",
      "  (611, 77)\t1\n",
      "  (611, 114)\t1\n",
      "  (611, 130)\t1\n",
      "  (611, 1371)\t1\n",
      "  (612, 292)\t1\n",
      "  (612, 688)\t1\n",
      "  (612, 974)\t1\n",
      "  (612, 1439)\t1\n",
      "  (612, 1491)\t1\n",
      "  (612, 1780)\t1\n",
      "  (612, 1985)\t1\n",
      "  (612, 2027)\t1\n",
      "  (612, 2740)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<613x2827 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9062 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bag_of_words = CountVectorizer()\n",
    "bag_of_words.fit(documents)\n",
    "word_counts = bag_of_words.transform(documents)\n",
    "\n",
    "print(word_counts)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `transform` method returns a sparse matrix. A sparse matrix is a more efficient manner of storing a matrix. If a matrix has mostly zero entries, it is better to just store the non-zero entries and their occurrence, their row and column. Sparse matrices have the method `toarray()` that returns a full matrix **but** doing so may result in memory issues. Some key hyperparameters of the `CountVectorizer` are shown below:\n",
    "\n",
    "* `min_df`: only counts words that appear in a minimum number of documents.\n",
    "* `max_df`: only counts words that do not appear more than a maximum number of documents.\n",
    "* `max_features`: limits the number of generated features, based on the frequency.\n",
    "\n",
    "After fitting a `CountVectorizer` object, the following method and attribute help with determining which index belongs to which word.\n",
    "\n",
    "* `get_feature_names()`: Returns a list of words used as features. The index of the word corresponds to the column index.\n",
    "* `vocabulary_`: A dictionary mapping a word to its corresponding feature index.\n",
    "\n",
    "Let's use `vocabulary_` to determine how many times \"programming\" occurs in the documents for Python the programming language and python the animal. Do the results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '10',\n",
       " '11',\n",
       " '111',\n",
       " '116',\n",
       " '12',\n",
       " '125',\n",
       " '13',\n",
       " '130',\n",
       " '14',\n",
       " '15',\n",
       " '1500',\n",
       " '158',\n",
       " '15806',\n",
       " '16',\n",
       " '165',\n",
       " '17',\n",
       " '18']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.vocabulary_['variable']\n",
    "bag_of_words.get_feature_names()[:18]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# get word counts\n",
    "counts_animal = bag_of_words.transform(animal_sents)\n",
    "counts_language = bag_of_words.transform(language_sents)\n",
    "\n",
    "# index for \"programming\"\n",
    "ind_programming = bag_of_words.vocabulary_['programming']\n",
    "\n",
    "# total counts across all documents\n",
    "print(counts_animal.sum(axis=0)[0, ind_programming])\n",
    "print(counts_language.sum(axis=0)[0, ind_programming])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `HashingVectorizer` transformer\n",
    "\n",
    "The `CountVectorizer` requires that we hold the mapping of words to features in memory. In addition, document processing cannot be parallelized because each worker needs to have the same mapping of word to column index. `CountVectorizer` objects are said to have _state_, they retain information of previous interactions and usage. A trick to improve the `CountVectorizer` is to use a hash function to convert the words into numbers. A hash function is a function that converts an input into a _deterministic_ value. In our context, we will use a hash function to convert a word into a number. The resulting number determines which feature column the word is mapped to. Python has a built-in hash function, seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3828753452135407626\n",
      "-8935813959752361631\n",
      "5749549707836713371\n",
      "-3828753452135407626\n",
      "-1737348948307885894\n",
      "13\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(hash(\"hi!\"))\n",
    "print(hash(\"python\"))\n",
    "print(hash(\"Python\"))\n",
    "print(hash(\"hi!\"))\n",
    "print(hash(\"13\"))\n",
    "print(hash(13))\n",
    "print(hash(31))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the function returns different values for different words. Also notice, the hash values of \"apple\" and \"apples\" are significantly different. Ideally no two inputs result in the same hash value, but this is impossible to avoid; when different inputs generate the same hash, it is referred to as a \"hash collision\".\n",
    "\n",
    "The `HashingVectorizer` class is similar to the `CountVectorizer` but it uses a hash function to render it *stateless*. The stateless nature of `HashingVectorizer` objects allows it to parallelize the counting process. There are two main disadvantages of `HashingVectorizer`:\n",
    "\n",
    "* Hash collisions are possible but in practice are often inconsequential.\n",
    "* Because the transformer is stateless, there is no mapping between word to feature index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<613x1048576 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 9062 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "hashing_bag_of_words = HashingVectorizer(norm=None) # by default, it normalizes the vectors\n",
    "hashing_bag_of_words.fit(documents)\n",
    "hashing_bag_of_words.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "HashingVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the feature matrix has over a million columns? This is in contrast from the result of the count vectorizer. The discrepancy is from the `HashingVectorizer` using, by default, $2^{20}=1048576$ different hash values to construct the count matrix. A vast majority of those indices will have no counts across all documents, and since we represent our feature matrix using a sparse matrix, we pay no cost for empty features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting time for CountVectorizer: 0.025290727615356445\n",
      "Fitting time for HashingVectorizer: 0.02378392219543457\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "t_0 = time.time()\n",
    "CountVectorizer().fit_transform(documents)\n",
    "t_elapsed = time.time() - t_0\n",
    "print(\"Fitting time for CountVectorizer: {}\".format(t_elapsed))\n",
    "\n",
    "t_0 = time.time()\n",
    "HashingVectorizer(norm=None).fit_transform(documents)\n",
    "t_elapsed = time.time() - t_0\n",
    "print(\"Fitting time for HashingVectorizer: {}\".format(t_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency\n",
    "\n",
    "Both the `CountVectorizer` and `HashingVectorizer` creates a feature matrix of raw counts. Using raw counts has two problems, documents vary widely in length and the counts will be large for common words such as \"the\" and \"is\". We need to use a weighting scheme that considers the aforementioned attributes. The term frequency-inverse document frequency, **tf-idf** for short, is a popular weighting scheme to improve the simple count based data from the bag of words model. It is the product of two values, the term frequency and the inverse document frequency. There are several variants but the most popular is defined below.\n",
    "\n",
    "* **Term Frequency:**\n",
    "$$ \\mathrm{tf}(t, d) = \\frac{\\mathrm{counts}(t, d)}{\\sqrt{\\sum_{t \\in d} \\mathrm{counts}(t, d)^2}}, $$\n",
    "    where $\\mathrm{counts}(t, d)$ is the raw count of term $t$ in document $d$ and $t \\in d$ are the terms in document $d$. The normalization results in a vector of unit length.\n",
    "\n",
    "* **Inverse Document Frequency:**\n",
    "$$ \\mathrm{idf}(t, D) = \\ln\\left(\\frac{\\text{number of documents in corpus } D}{1 + \\text{number of documents with term } t}\\right). $$\n",
    "    Every counted term $t$ in the corpus will have its own idf weight. The $1+$ in the denominator is to ensure no division by zero if a term does not appear in the corpus. The idf weight is simply the log of the inverse of a term's document frequency.\n",
    "    \n",
    "With both $\\mathrm{tf}(t, d)$ and $\\mathrm{idf}(t, D)$ calculated, the tf-idf weight is\n",
    "\n",
    "$$ \\mathrm{tfidf}(t, d, D) = \\mathrm{tf}(t, d) \\mathrm{idf}(t, D).$$\n",
    "\n",
    "With the idf weighting, words that are very common throughout the documents get weighted down. The reverse is true; the count of rare words get weighted up. With the tf-idf weighting scheme, a machine learning model will have an easier time to learn patterns to properly predict labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to apply the tf-idf weighting in `scikit-learn`, differing in what input they work on. `TfidfVectorizer` works on an array of documents (e.g., list of sentences) while the `TfidfTransformer` works on a count matrix, like the outputs of `HashingVectorizer` and `CountVectorizer`. `TfidfVectorizer` encapsulates the `CountVectorizer` and `TfidfTransformer` into one class. Since we have already calculated the word counts, we will demonstrate the `TfidfTransformer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2591)\t0.10825603988185362\n",
      "  (0, 2554)\t0.17902371969996278\n",
      "  (0, 2382)\t0.26992506013840034\n",
      "  (0, 2372)\t0.31949864848142795\n",
      "  (0, 2371)\t0.30495846976061325\n",
      "  (0, 2349)\t0.20146290031439962\n",
      "  (0, 2152)\t0.27667407406565936\n",
      "  (0, 2150)\t0.19532901600414582\n",
      "  (0, 2030)\t0.3399918793828262\n",
      "  (0, 2027)\t0.09760050947131484\n",
      "  (0, 1698)\t0.27667407406565936\n",
      "  (0, 1545)\t0.29368023094352563\n",
      "  (0, 1370)\t0.12314122012194885\n",
      "  (0, 1278)\t0.1100941524948255\n",
      "  (0, 986)\t0.3399918793828262\n",
      "  (0, 278)\t0.30495846976061325\n",
      "  (0, 227)\t0.09595670519828554\n",
      "  (1, 2793)\t0.2657073440729373\n",
      "  (1, 2770)\t0.2584299357230377\n",
      "  (1, 2554)\t0.16721873392438288\n",
      "  (1, 2349)\t0.18817825470149174\n",
      "  (1, 2080)\t0.2743146912560042\n",
      "  (1, 1789)\t0.1665640887299626\n",
      "  (1, 1775)\t0.10358981605831621\n",
      "  (1, 1517)\t0.2743146912560042\n",
      "  :\t:\n",
      "  (607, 65)\t0.5484553012496333\n",
      "  (608, 2484)\t0.6443782471805386\n",
      "  (608, 1570)\t0.5566049188894946\n",
      "  (608, 51)\t0.5243735680109705\n",
      "  (609, 2027)\t0.1902259424288321\n",
      "  (609, 1985)\t0.4138467984548892\n",
      "  (609, 1278)\t0.2145763790340346\n",
      "  (609, 844)\t0.5544301811881516\n",
      "  (609, 71)\t0.662653054006406\n",
      "  (610, 2750)\t0.5773502691896258\n",
      "  (610, 1978)\t0.5773502691896258\n",
      "  (610, 161)\t0.5773502691896258\n",
      "  (611, 1371)\t0.44631466761823324\n",
      "  (611, 130)\t0.44631466761823324\n",
      "  (611, 114)\t0.5484553012496333\n",
      "  (611, 77)\t0.5484553012496333\n",
      "  (612, 2740)\t0.4282958709135864\n",
      "  (612, 2027)\t0.12294968715578693\n",
      "  (612, 1985)\t0.2674836762577159\n",
      "  (612, 1780)\t0.3841634501261973\n",
      "  (612, 1491)\t0.3841634501261973\n",
      "  (612, 1439)\t0.23755872161717845\n",
      "  (612, 974)\t0.3699559839793457\n",
      "  (612, 688)\t0.4282958709135864\n",
      "  (612, 292)\t0.24082454641596665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "tfidf_weights = tfidf.fit_transform(word_counts)\n",
    "print(tfidf_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We no longer have raw counts in our feature matrix. Let's use the `idf_` attribute of the fitted tf-idf transformer to inspect the top idf weights and their corresponding terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i dont understand what this is actually doing\n",
    "tfidf.get_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.726847747587197 zope\n",
      "6.726847747587197 hall\n",
      "6.726847747587197 greatest\n",
      "6.726847747587197 greatly\n",
      "6.726847747587197 gripped\n",
      "6.726847747587197 gripping\n",
      "6.726847747587197 groovy\n",
      "6.726847747587197 grow\n",
      "6.726847747587197 growing\n",
      "6.726847747587197 grumpy\n",
      "6.726847747587197 guard\n",
      "6.726847747587197 guide\n",
      "6.726847747587197 guinea\n",
      "6.726847747587197 gutted\n",
      "6.726847747587197 göttingen\n",
      "6.726847747587197 habitation\n",
      "6.726847747587197 hamilton\n",
      "6.726847747587197 frustrations\n",
      "6.726847747587197 handle\n"
     ]
    }
   ],
   "source": [
    "top_idf_indices = tfidf.idf_.argsort()[:-20:-1]\n",
    "ind_to_word = bag_of_words.get_feature_names()\n",
    "\n",
    "for ind in top_idf_indices:\n",
    "    print(tfidf.idf_[ind], ind_to_word[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using tf-idf weighting renders the process as _stateful_; to apply the idf weight, we need to know the frequency of each word across all documents. While we may initially use `HasingVectorizer` to have a stateless transformer, coupling it with `TfidfTransformer` will create a stateful process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving signal\n",
    "\n",
    "So far, we have discussed how using tf-idf rather than raw counts will improve the performance of our machine learning model. There are several other approaches that can boost performance; we will discuss techniques that improve the signal in our data set. Note, the following techniques may marginally increase model performance. It may be best to create a baseline model and measure the increased performance with the new model additions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Words such as \"the\", \"a\", and \"or\" are so common throughout our corpus that they do not contribute any signal to our data set. Further, omitting these words will reduce our already high dimensional data set. It is best to not have these words as features and not be counted in the analysis. The set of words that will not factor into our analysis are called **stop words**.\n",
    "\n",
    "spaCy provides a `set` of around 300 commonly used English words. When using stop words, it is best to examine the entries in case there are certain words you want to be included or not included. Since the words are provided as a Python `set`, we can use methods available to `set` objects to modify entries of the `set` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'set'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'python',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "print(type(STOP_WORDS))\n",
    "STOP_WORDS_python = STOP_WORDS.union({\"python\"})\n",
    "STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"python\" in STOP_WORDS_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "In our current analysis, words like \"python\" and \"pythons\" will be counted as separate words. We understand that they represent the same concept and want them to be treated as the same word. The same applies to other words like \"run\", \"runs\", \"ran\", and \"running\", they all represent the same meaning. **Stemming** is the process of reducing a word to its stem. Note, the stemming process is not 100% effective and sometimes the resulting stem is not an actual word. For example, the popular Porter stemming algorithm applied to \"argues\" and \"arguing\" returns `\"argu\"`.\n",
    "\n",
    "**Lemmatization** is the process of reducing a word to its lemma, or the dictionary form of the word. It is a more sophisticated process than stemming as it considers context and part of speech. Further, the resulting lemma is an actual word. spaCy does not have a stemming algorithm but does offer lemmatization. Each word analyzed by spaCy has the attribute `lemma_` which returns the lemma of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'run', 'run']\n",
      "['buy', 'buy', 'buying', 'buy']\n",
      "['see', 'saw', 'see', 'see']\n"
     ]
    }
   ],
   "source": [
    "print([word.lemma_ for word in nlp('run runs ran running')])\n",
    "print([word.lemma_ for word in nlp('buy buys buying bought')])\n",
    "print([word.lemma_ for word in nlp('see saw seen seeing')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: As of version 2.0.16 of spaCy, there is the bug with the English lemmatization and will fail in instances it should not. However, the bug has been fixed and a patch will be included in a future update, version 2.1.x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply lemmatization in `scikit-learn`, you need to pass a function to the keyword `tokenizer` of whatever text vectorizer you are deploying. See the example below were we apply lemmatization for a `TfidfVectorizer` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfidfVectorizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', '\\n\\n', '\\n\\n\\n', ' ', '\"', \"'\", \"'s\", '(', ')', ',', '-', '.', '1', '2', '3', ':', ';', '<', '=', 'allow', 'b', 'ball', 'block', 'body', 'breed', 'c', 'captivity', 'class', 'code', 'common', 'compile', 'cpython', 'describe', 'design', 'development', 'division', 'e.g.', 'eat', 'egg', 'example', 'expression', 'feature', 'female', 'find', 'ft', 'function', 'good', 'human', 'implementation', 'include', 'integer', 'island', 'java', 'kill', 'language', 'large', 'later', 'length', 'library', 'like', 'list', 'long', 'm', 'm.', 'male', 'measure', 'method', 'module', 'new', 'number', 'object', 'old', 'operator', 'pattern', 'prey', 'program', 'programming', 'provide', 'r.', 'range', 'reference', 'release', 'report', 'reticulate', 'reticulated', 'small', 'snake', 'standard', 'statement', 'string', 'support', 'syntax', 'system', 'time', 'type', 'value', 'variable', 'version', 'write', 'year']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "# we need to generate the lemmas of the stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS) # nlp function needs a string\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "tfidf_lemma = TfidfVectorizer(max_features=100, \n",
    "                              stop_words=stop_words_lemma.union({\"python\"}),\n",
    "                              tokenizer=lemmatizer)\n",
    "\n",
    "tfidf_lemma.fit(documents)\n",
    "print(tfidf_lemma.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and n-grams\n",
    "\n",
    "Tokenization refers to dividing up a document into pieces to be counted. In our analysis so far, we are only counting words. However, it may be useful to count a sequence of words such as \"natural environment\" and \"virtual environment\". Counting these **bigrams** for our word usage analyzer may boost performance. More generally, an n-gram refers to the n sequence of words. In `scikit-learn`, n-grams can be included by setting `ngram_range=(min_n, max_n)` for the vectorizer, where `min_n` and `max_n` are the lower and upper bound of the range of n-grams to include. For example, `ngram_range=(1, 2)` will include words and bigrams while `ngram_range=(2, 2)` will only count bigrams. Let's see what are the most frequent bigrams in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23 ft',\n",
       " 'auliya et',\n",
       " 'ball care',\n",
       " 'ball pythons',\n",
       " 'et al',\n",
       " 'floating point',\n",
       " 'ft length',\n",
       " 'integer division',\n",
       " 'isbn 978',\n",
       " 'list comprehensions',\n",
       " 'object oriented',\n",
       " 'oriented programming',\n",
       " 'programming language',\n",
       " 'programming languages',\n",
       " 'reference implementation',\n",
       " 'reticulated pythons',\n",
       " 'scripting language',\n",
       " 'standard library',\n",
       " 'van rossum',\n",
       " 'year old']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_counter=CountVectorizer(max_features=20, ngram_range=(2,2), stop_words=STOP_WORDS.union({\"python\"}))\n",
    "bigram_counter.fit(documents)\n",
    "\n",
    "bigram_counter.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questions**\n",
    "* Is using stop words more important when using `CountVectorizer`/`HashingVectorizer` or when using the `TfidfVectorizer`?\n",
    "* Is it practical to use a large n-gram range, for example, count 3-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**answer**\n",
    "1. Tfidf already has a way to weigh-down the STOP_WORDS i.e its going to natuarally handle those STOP_WORDS, so its more useful in Count/Hashing Vectorizer\n",
    "\n",
    "2. 3-grams range or more-grams are usable but its better not not use it cause it not usually nor a good method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document similarity\n",
    "\n",
    "After we have transformed our corpus into a matrix, we can interpret our data set as representing a set of vectors in a $p$-dimensional space, where each document is its own vector. One common analysis is to find similar documents. The cosine similarity is a metric that measure how well aligned in space are two vectors, equal to the cosine of the angle in between the two vectors. If the vectors are perfectly aligned, they point in the same direction, the angle they form is 0 and the similarity score is 1. If the vectors are orthogonal, forming an angle of 90 degrees, the similarity metric is 0. Mathematically, the cosine similarity metric is equal to the dot product of two vectors, normalized,\n",
    "\n",
    "$$ \\frac{v_1 \\cdot v_2}{\\|v_1 \\|\\|v_2 \\|}, $$\n",
    "\n",
    "where $v_1$ and $v_2$ are two document vectors and $\\| v_1 \\|$ and $\\| v_2 \\|$ are their lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word usage classifier\n",
    "\n",
    "Let's build a word usage classifier with all the techniques we have seen. The model will include:\n",
    "\n",
    "* tf-idf weighting\n",
    "* stop words\n",
    "* words and bigrams\n",
    "* lemmatization\n",
    "\n",
    "Applying the above techniques should result in a data set with enough signal that a machine learning model can learn from. For this exercise, we will use the naive Bayes model; a probabilistic model that calculates conditional probabilities using Bayes theorem. The term naive is applied because it assumes the features are conditionally independent from each other. You can think of a naive Bayes classifier working by determining what class should a document be assigned based upon the frequencies of words in the different classes in the training set. Naive Bayes is often used as benchmark model for NLP as it is quick to train. More about the model in general can be found [here](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) and details of the `scikit-learn` implementation is found [here](https://scikit-learn.org/stable/modules/naive_bayes.html). After training our model, we will see how well it performs for a chosen set of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['-pron-'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9902120717781403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# create data set and labels\n",
    "documents = animal_sents + language_sents\n",
    "labels = [\"animal\"]*len(animal_sents) + [\"language\"]*len(language_sents)\n",
    "\n",
    "# lemma of stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS)\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "# create and train pipeline\n",
    "tfidf = TfidfVectorizer(stop_words=stop_words_lemma, tokenizer=lemmatizer, ngram_range=(1, 2))\n",
    "pipe = Pipeline([('vectorizer', tfidf), ('classifier', MultinomialNB())])\n",
    "pipe.fit(documents, labels)\n",
    "\n",
    "print(\"Training accuracy: {}\".format(pipe.score(documents, labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Python program is only 100 bytes long. --> language at 70.8075%\n",
      "A python's bite is not venomous but still hurts. --> language at 59.9965%\n",
      "I can't find the error in the python code. --> language at 71.1411%\n",
      "Where is my pet python; I can't find her! --> animal at 53.5441%\n",
      "I use for and while loops when writing Python. --> language at 84.014%\n",
      "The python will loop and wrap itself onto me. --> language at 64.8318%\n",
      "I use snake case for naming my variables. --> language at 56.8347%\n",
      "My python has grown to over 10 ft long! --> animal at 63.218%\n",
      "I use virtual environments to manage package versions. --> language at 77.3986%\n",
      "Pythons are the largest snakes in the environment. --> animal at 71.3702%\n"
     ]
    }
   ],
   "source": [
    "test_docs = [\"My Python program is only 100 bytes long.\",\n",
    "             \"A python's bite is not venomous but still hurts.\",\n",
    "             \"I can't find the error in the python code.\",\n",
    "             \"Where is my pet python; I can't find her!\",\n",
    "             \"I use for and while loops when writing Python.\",\n",
    "             \"The python will loop and wrap itself onto me.\",\n",
    "             \"I use snake case for naming my variables.\",\n",
    "             \"My python has grown to over 10 ft long!\",\n",
    "             \"I use virtual environments to manage package versions.\",\n",
    "             \"Pythons are the largest snakes in the environment.\"]\n",
    "\n",
    "class_labels = [\"animal\", \"language\"]\n",
    "y_proba = pipe.predict_proba(test_docs)\n",
    "predicted_indices = (y_proba[:, 1] > 0.5).astype(int)\n",
    "\n",
    "for i, index in enumerate(predicted_indices):\n",
    "    print(test_docs[i], \"--> {} at {:g}%\".format(class_labels[index], 100*y_proba[i, index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Encapsulate the entire process of gathering a corpus, constructing, and training a model into a function. Afterwards, deploy the model to other sets of homonyms.\n",
    "1. Measure the model's improvements by stripping out things such as the use of stop words and lemmatization. Perhaps you can incorporate model additions as parameters to the previously mentioned function. What model additions increases the performance the most?\n",
    "1. Consider another source of data and see how well the model performs with the new corpus.\n",
    "1. Naive Bayes classifier calculates conditional probabilities from the training set. In other words, it determines values like $P(\\text{snake | }Y = \\text{animal})$, the probability a document has the word \"snake\" given if the document belongs to those of python the animal. These values are stored in `coef_` attribute of a trained naive Bayes model. Can you use these coefficients to determine the most discriminative features? In other words, what terms when found in a document really help classify the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import spacy\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pages_to_sentences(*pages):\n",
    "    \"\"\"Return a list of sentences in Wikipedia articles.\"\"\"\n",
    "    sentences = []\n",
    "    \n",
    "    for page in pages:\n",
    "        p = wikipedia.page(page)\n",
    "        doc = nlp(p.content)\n",
    "        sentences += [sent.text for sent in doc.sents]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "def lemmatizer(text):\n",
    "    return [word.lemma_ for word in nlp(text)]\n",
    "\n",
    "# we need to generate the lemmas of the stop words\n",
    "stop_words_str = \" \".join(STOP_WORDS) # nlp function needs a string\n",
    "stop_words_lemma = set(word.lemma_ for word in nlp(stop_words_str))\n",
    "\n",
    "\n",
    "def get_corpus(words):\n",
    "    corpus=[]\n",
    "    labels=[]\n",
    "    \n",
    "    for k,v in words.items():\n",
    "        doc = pages_to_sentences(*v)\n",
    "        corpus += doc\n",
    "        labels += [k]*len(doc)\n",
    "        \n",
    "    return corpus, labels\n",
    "\n",
    "\n",
    "amazon = {\"greek\": [\"Amazons\"],\n",
    "          \"rainforest\": [\"Amazon_rainforest\"],\n",
    "          \"country\": [\"Amazon_(company)\"]}\n",
    "\n",
    "python = {\"animal\":[\"Reticulated python\", \"Ball Python\"],\n",
    "          \"language\":[ \"Python (programming language)\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus, labels = get_corpus(amazon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(documents, labels, test_size=0.2, random_state=2)\n",
    "\n",
    "model = Pipeline([(\"vectorizer\", TfidfVectorizer()), (\"classifier\", MultinomialNB())])\n",
    "\n",
    "param_grid = {\"vectorizer__ngram_range\": [(1,1), (1,2), (2,2)],\n",
    "             \"vectorizer__tokenizer\": [None, lemmatizer]\n",
    "#             \"vectorizer__stop_words\": [None, stop_words_lemma] #using stop_words caused an error, i'll troubleshoot this later\n",
    "             }\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, verbose=1)\n",
    "\n",
    "grid_search.fit(corpus, labels)\n",
    "grid_search.cv_results_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'vectorizer__ngram_range': (1, 1), 'vectorizer__tokenizer': None}\n",
      "\n",
      "score:  0.8384991843393148\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)\n",
    "print()\n",
    "print(\"score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2, 613]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-05725fc3b70c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2, 613]"
     ]
    }
   ],
   "source": [
    "grid_search.fit(python, labels)\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "print()\n",
    "print(\"score: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright &copy; 2020 The Data Incubator.  All rights reserved.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbclean": true
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
